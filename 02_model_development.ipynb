{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84e3b4f8",
   "metadata": {},
   "source": [
    "# Project: Diagnostic reasoning assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0273d26",
   "metadata": {},
   "source": [
    "**Author:** Julia Parnis   \n",
    "**Date:** February 14, 2026  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753bd562",
   "metadata": {},
   "source": [
    "## Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541bbf45",
   "metadata": {},
   "source": [
    "**Goal:** Build an AI-powered diagnostic assistant that provides ranked differential diagnoses through iterative questioning, with enhanced detection of rare and overlooked conditions.\n",
    "\n",
    "**Key Innovation:** Two-tier ML architecture combined with RAG (Retrieval-Augmented Generation) for transparent clinical reasoning and literature-backed rare disease identification.\n",
    "\n",
    "**Dataset:** DDXPlus - 1.3M synthetic patient cases, 49 pathologies  \n",
    "**Source:** [Hugging Face](https://huggingface.co/datasets/aai530-group6/ddxplus)\n",
    "**Citation:** Fansi Tchango et al. (2022)\n",
    "\n",
    "**Note:** This is a synthetic dataset (computer-generated from medical knowledge bases) designed for research and education. It provides a robust, privacy-compliant foundation for developing diagnostic AI systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202fd173",
   "metadata": {},
   "source": [
    "# Notebook 2: Multi-Class Diagnosis Prediction - Baseline Models & Optimization \n",
    "\n",
    "**Objective:** Develop and evaluate machine learning models to predict patient pathology (49-class classification) using filtered evidence features, with emphasis on Top-k accuracy metrics and class imbalance handling.\n",
    "\n",
    "**Success Criteria:**\n",
    "- Establish strong internal baselines (frequency baseline → dummy → logistic regression)\n",
    "- Beat simple baselines on Top‑k metrics (Top‑1/Top‑3/Top‑5)\n",
    "- Demonstrate a leakage-safe preprocessing pipeline based on EDA findings\n",
    "- Show meaningful improvement on macro metrics (macro F1 / per-class recall), especially for rare diseases\n",
    "- Optional: compare to published benchmarks only when metrics + setup are directly comparable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bab1dd1",
   "metadata": {},
   "source": [
    "## **Tasks**  \n",
    "\n",
    "### **Phase 0: Setup and Reproducibility**\n",
    "Project setup  \n",
    "- Fix random seeds (NumPy / sklearn)\n",
    "- Define a small configuration block (paths, flags: use_unique_cases, use_value_tokens, use_sample_weights)\n",
    "- Create output folders (outputs/, models/, figures/)\n",
    "- Log key dataset stats (rows, unique cases, split sizes) for traceability \n",
    "\n",
    "### **Phase 1: Foundation (Based on EDA Findings)**\n",
    "Preprocessing pipeline implementation\n",
    "- Parse list columns (EVIDENCES, DIFFERENTIAL_DIAGNOSIS) into Python lists\n",
    "- Default-value filtering (correct rule):\n",
    "    - Filter value-coded items only when value == default_value from the evidence mapping\n",
    "    - Do not hard-code *_@_0 (defaults are evidence-specific and not always V_0) \n",
    "- Validation target:\n",
    "    - mean effective base evidences ≈ 13.56 when weighted to match original distribution \n",
    "    - mean effective base evidences ≈ 13.65 on unique patterns (EDA focus) \n",
    "- Duplicate pattern handling with frequency preservation\n",
    "    - Create case_hash (signature) using only model-available fields\n",
    "    - Deduplicate to unique patterns but keep frequency as metadata (optional training weight)\n",
    "- Leakage-safe train/val/test\n",
    "    - Ensure no case_hash overlaps across splits (train∩val∩test must be empty)\n",
    "    - Prefer “keep test fixed, remove overlaps from train/val” to preserve the official test set\n",
    "- Validate preprocessing against EDA\n",
    "    - counts: total unique cases ≈ 1,278,666 \n",
    "    - effective evidence stats: min/max should stay within [1,36] \n",
    "    - multi-choice behavior: extra_multi_values mean ≈ 4.39"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eef2c11",
   "metadata": {},
   "source": [
    "### **Phase 2: Feature Engineering**\n",
    "**Feature engineering and ML-ready matrices**\n",
    "- Baseline encoding (recommended first): base-level effective evidences\n",
    "    - Multi-hot encode the effective base codes (223 features) \n",
    "- Demographics\n",
    "    - Encode SEX (one-hot)\n",
    "    - Use AGE numeric + optionally an AGE_GROUP categorical feature (binning is useful for interpretation)\n",
    "- Simple derived numeric features\n",
    "    - num_evidences_effective (base concepts)\n",
    "    - num_items_effective (value items)\n",
    "    - extra_multi_values\n",
    "- Document feature meaning\n",
    "    - Add one small table describing each feature group and why it exists.\n",
    "\n",
    "**Optional (Phase 2b, after baseline works):**\n",
    "- Add value-level tokens (e.g., E_54=V_161) to preserve signal from categorical/multi-choice items (still filtering defaults). This expands feature count but often improves performance.\n",
    "\n",
    "Why: base-level features are strong, interpretable, and lightweight. Value tokens are a second step when you want extra signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aca54e0",
   "metadata": {},
   "source": [
    "## Phase 0: Setup and Reproducibility\n",
    "\n",
    "**Purpose:** Establish reproducible environment and configuration for all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "322d9f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\julia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 0.1 Import Core Libraries \n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import pickle  # ok,  for sklearn models joblib is often better (optional)\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from datasets import load_dataset\n",
    "import ast\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cycler import cycler\n",
    "\n",
    "# Scikit-learn: preprocessing + modeling\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder, MultiLabelBinarizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Scikit-learn: model selection\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# Scikit-learn: metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    top_k_accuracy_score\n",
    ")\n",
    "\n",
    "# Models\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB  # better for binary multi-hot features\n",
    "\n",
    "# Silence noisy warnings (optional)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Optional: Gradient Boosting libraries\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LIGHTGBM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LIGHTGBM_AVAILABLE = False\n",
    "    print(\"⚠️ LightGBM not installed. (Optional) Install with: pip install lightgbm\")\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"⚠️ XGBoost not installed. (Optional) Install with: pip install xgboost\")\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ac34e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Random seed set to: 42\n",
      "✓ Experiment: exp_20260214_2013\n",
      "✓ Models to train (6): dummy_most_frequent, dummy_stratified, logistic_regression, bernoulli_nb, random_forest, lightgbm\n",
      "✓ Preprocessing: unique_cases=True, encoding=base\n",
      "✓ Output directories created:\n",
      "    outputs\\exp_20260214_2013\n",
      "    models\\exp_20260214_2013\n",
      "    figures\\exp_20260214_2013\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "0.2: Setup - Seeds, Config, and Directories\n",
    "\"\"\"\n",
    "\n",
    "# ===== RANDOM SEEDS (for reproducibility) =====\n",
    "random_seed = 42\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(random_seed)  # Makes Python hashing deterministic\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "print(f\"✓ Random seed set to: {random_seed}\")\n",
    "\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    Central configuration for this experiment.\n",
    "    Change settings here to control preprocessing and modeling.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Data paths ---\n",
    "    data_dir = Path(\"data\")\n",
    "    evidence_map_path = data_dir / \"release_evidences.json\"\n",
    "    conditions_map_path = data_dir / \"release_conditions.json\"\n",
    "\n",
    "    # --- Output folders ---\n",
    "    output_root = Path(\"outputs\")\n",
    "    models_root = Path(\"models\")\n",
    "    figures_root = Path(\"figures\")\n",
    "    experiment_name = f\"exp_{datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
    "\n",
    "    # Will be set after folder creation:\n",
    "    output_dir = None\n",
    "    models_dir = None\n",
    "    figures_dir = None\n",
    "\n",
    "    # --- Preprocessing settings ---\n",
    "    use_unique_cases = True       # deduplicate by case signature (unique patterns)\n",
    "    use_sample_weights = True     # use frequency as training weight (NOT a feature)\n",
    "\n",
    "    # IMPORTANT: default filtering rule (from EDA)\n",
    "    # Do NOT hard-code '*_@_0'. Defaults are evidence-specific.\n",
    "    # Correct rule: remove value-coded items when value == evidence_map[base][\"default_value\"].\n",
    "\n",
    "    filter_defaults = True        # Remove default-valued evidences \n",
    "    feature_encoding = \"base\"     # # \"base\" (E_66) OR \"token\" (E_54=V_161) later\n",
    "\n",
    "    # --- Modeling settings ---\n",
    "    class_weight = \"balanced\"     # Handle class imbalance (254:1 ratio), works for LogisticRegression / RandomForest; NB ignores it\n",
    "    top_k = [1, 3, 5]            # Evaluation metrics\n",
    "    n_jobs = -1                   # Use all CPU cores\n",
    "    random_seed = random_seed\n",
    "\n",
    "    # --- Models to train ---\n",
    "    models_to_train = [\n",
    "        \"dummy_most_frequent\",    # Baseline: always predict most common class\n",
    "        \"dummy_stratified\",       # Baseline: random predictions weighted by class frequency\n",
    "        \"logistic_regression\",    # Simple, interpretable (good starting point)\n",
    "        \"bernoulli_nb\",          # Fast, works well with binary sparse features\n",
    "        \"random_forest\",         # Captures non-linear patterns\n",
    "        \"lightgbm\",             # Expected best performance for tabular data\n",
    "        # \"xgboost\",            # Uncomment to compare with LightGBM\n",
    "    ]\n",
    "\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# ===== CREATE EXPERIMENT DIRECTORIES =====\n",
    "config.output_root.mkdir(exist_ok=True)\n",
    "config.models_root.mkdir(exist_ok=True)\n",
    "config.figures_root.mkdir(exist_ok=True)\n",
    "\n",
    "config.output_dir = config.output_root / config.experiment_name\n",
    "config.models_dir = config.models_root / config.experiment_name\n",
    "config.figures_dir = config.figures_root / config.experiment_name\n",
    "\n",
    "config.output_dir.mkdir(exist_ok=True)\n",
    "config.models_dir.mkdir(exist_ok=True)\n",
    "config.figures_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# ===== AUTO-REMOVE UNAVAILABLE LIBRARIES =====\n",
    "if \"lightgbm\" in config.models_to_train and not LIGHTGBM_AVAILABLE:\n",
    "    config.models_to_train.remove(\"lightgbm\")\n",
    "    print(\"⚠️  Removed 'lightgbm' from training (not installed)\")\n",
    "\n",
    "if \"xgboost\" in config.models_to_train and not XGBOOST_AVAILABLE:\n",
    "    config.models_to_train.remove(\"xgboost\")\n",
    "    print(\"⚠️  Removed 'xgboost' from training (not installed)\")\n",
    "\n",
    "# ===== SUMMARY =====\n",
    "print(f\"✓ Experiment: {config.experiment_name}\")\n",
    "print(f\"✓ Models to train ({len(config.models_to_train)}): {', '.join(config.models_to_train)}\")\n",
    "print(f\"✓ Preprocessing: unique_cases={config.use_unique_cases}, encoding={config.feature_encoding}\")\n",
    "print(f\"✓ Output directories created:\")\n",
    "print(f\"    {config.output_dir}\")\n",
    "print(f\"    {config.models_dir}\")\n",
    "print(f\"    {config.figures_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19f65f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Display settings configured\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "0.3 Display Settings\n",
    "\"\"\"\n",
    "\n",
    "# Pandas display - useful for inspecting dataframes\n",
    "pd.set_option(\"display.max_columns\", 20)\n",
    "pd.set_option(\"display.max_rows\", 60)\n",
    "pd.set_option(\"display.max_colwidth\", 120)\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "\n",
    "print(\"✓ Display settings configured\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "961a470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.4 - Visualization Settings\n",
    "\n",
    "# Apply clean seaborn style\n",
    "plt.style.use('seaborn-v0_8-white')\n",
    "\n",
    "# IBM Design colorblind-safe palette\n",
    "IBM_COLORS = {\n",
    "    'blue': '#648FFF',\n",
    "    'purple': '#785EF0', \n",
    "    'magenta': '#DC267F',\n",
    "    'orange': '#FE6100',\n",
    "    'yellow': '#FFB000',\n",
    "    'teal': '#06A39B',\n",
    "    'gray': '#5F6368'\n",
    "}\n",
    "\n",
    "# Figure defaults (presentation-optimized)\n",
    "plt.rcParams.update({\n",
    "    # Figure size and resolution\n",
    "    \"figure.figsize\": (6, 4),          # Good for 2+ figs per slide\n",
    "    \"figure.dpi\": 120,                 # Screen display\n",
    "    \"savefig.dpi\": 300,                # High-quality save\n",
    "\n",
    "    # Font sizes (readable on Zoom)\n",
    "    \"axes.titlesize\": 16,\n",
    "    \"axes.labelsize\": 14,\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12,\n",
    "    \"legend.fontsize\": 12,\n",
    "\n",
    "    # Appearance (subtle, professional)\n",
    "    \"axes.edgecolor\": IBM_COLORS[\"gray\"],\n",
    "    \"axes.linewidth\": 1.2,\n",
    "    \"grid.color\": \"#D9D9D9\",\n",
    "    \"grid.linewidth\": 0.8,\n",
    "    \"grid.alpha\": 0.6,\n",
    "})\n",
    "\n",
    "# Set IBM color cycle (for multi-line plots)\n",
    "plt.rcParams[\"axes.prop_cycle\"] = cycler(color=[\n",
    "    IBM_COLORS[\"blue\"],\n",
    "    IBM_COLORS[\"teal\"],\n",
    "    IBM_COLORS[\"purple\"],\n",
    "    IBM_COLORS[\"magenta\"],\n",
    "    IBM_COLORS[\"orange\"],\n",
    "])\n",
    "\n",
    "def save_fig(fig, filename: str):\n",
    "    \"\"\"Save a figure into this experiment's figures folder.\"\"\"\n",
    "    # Use experiment-specific folder created in config\n",
    "    out_dir = getattr(config, \"figures_dir\", Path(\"figures\"))\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    path = out_dir / filename\n",
    "    fig.savefig(path, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "    print(f\"✓ Saved: {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a1d6fc",
   "metadata": {},
   "source": [
    "## Phase 1: Foundation/ Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce70fc64",
   "metadata": {},
   "source": [
    "### 1.1 Pre-processing helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad9bf758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evidence parsing helpers loaded\n",
      "  Functions: split_ev, is_effective_item, to_item_token\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "===============================================================================\n",
    "1.1a: Evidence Parsing & Default-Filtering Helpers\n",
    "===============================================================================\n",
    "DDXPlus encodes evidences in two main formats:\n",
    "\n",
    "1) Binary evidence (no value):\n",
    "   - Example: \"E_66\"  -> means the evidence is present (e.g., \"shortness of breath\")\n",
    "\n",
    "2) Value-coded evidence (categorical / multi-choice):\n",
    "   - Example: \"E_54_@_V_161\"  -> base code is \"E_54\", value code is \"V_161\"\n",
    "   - These evidences have a \"default_value\" in the official mapping file that represents\n",
    "     \"not present / not applicable / negative\". Those default-valued entries must be filtered\n",
    "     out to avoid treating negatives as positives.\n",
    "\n",
    "Important:\n",
    "- Filtering defaults reduces counts somewhat.\n",
    "- Separately, collapsing multi-choice values to unique base codes reduces counts further.\n",
    "  (So: raw item count ≠ effective item count ≠ effective base-code count.)\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "\n",
    "def split_ev(item: str) -> Tuple[str, Optional[str]]:\n",
    "    \"\"\"\n",
    "    Split an evidence string into (base_code, value_code_or_None).\n",
    "\n",
    "    Examples:\n",
    "        >>> split_ev(\"E_54_@_V_161\")\n",
    "        (\"E_54\", \"V_161\")\n",
    "        >>> split_ev(\"E_66\")\n",
    "        (\"E_66\", None)\n",
    "    \"\"\"\n",
    "    if \"_@_\" in item:\n",
    "        base, value = item.split(\"_@_\", 1)\n",
    "        return base, value\n",
    "    return item, None\n",
    "\n",
    "\n",
    "def is_default_value(base: str, value: str, evidences_map: Dict) -> bool:\n",
    "    \"\"\"\n",
    "    Return True if (base, value) equals the evidence's default value.\n",
    "\n",
    "    Why we need this:\n",
    "    - For value-coded evidences, the dataset may store the default value explicitly.\n",
    "      Example: travel history might be stored as \"E_204_@_V_0\" for \"no travel\".\n",
    "      If we keep that, it looks like *everyone* has travel history recorded,\n",
    "      which is misleading for ML features.\n",
    "\n",
    "    Robustness:\n",
    "    - In some datasets/mappings, the default might be stored as \"0\" while the data stores \"V_0\",\n",
    "      or vice versa. This function treats those as equivalent.\n",
    "\n",
    "    Args:\n",
    "        base: evidence base code (e.g., \"E_204\")\n",
    "        value: value code extracted from the item (e.g., \"V_0\" or \"0\")\n",
    "        evidences_map: mapping loaded from release_evidences.json\n",
    "\n",
    "    Returns:\n",
    "        True if the value is the default and should be filtered out.\n",
    "    \"\"\"\n",
    "    default = evidences_map.get(base, {}).get(\"default_value\", None)\n",
    "    if default is None:\n",
    "        return False\n",
    "\n",
    "    default_str = str(default)\n",
    "    value_str = str(value)\n",
    "\n",
    "    # Consider equivalent representations: \"0\" <-> \"V_0\"\n",
    "    candidates = {default_str}\n",
    "    if default_str.startswith(\"V_\") and default_str[2:].isdigit():\n",
    "        candidates.add(default_str[2:])          # \"V_0\" -> \"0\"\n",
    "    if (not default_str.startswith(\"V_\")) and default_str.isdigit():\n",
    "        candidates.add(f\"V_{default_str}\")       # \"0\" -> \"V_0\"\n",
    "\n",
    "    return value_str in candidates\n",
    "\n",
    "\n",
    "def is_effective_item(item: str, evidences_map: Dict) -> bool:\n",
    "    \"\"\"\n",
    "    Decide whether an evidence item should be kept (\"effective\") for ML.\n",
    "\n",
    "    Rules:\n",
    "    - Binary evidence (no value): keep if present in the list\n",
    "    - Value-coded evidence: keep only if value != default_value\n",
    "    \"\"\"\n",
    "    base, value = split_ev(item)\n",
    "    if value is None:\n",
    "        return True\n",
    "    return not is_default_value(base, value, evidences_map)\n",
    "\n",
    "\n",
    "def to_item_token(item: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert evidence entry into a token suitable for ML feature encoding.\n",
    "\n",
    "    - Binary:        \"E_66\"         -> \"E_66\"\n",
    "    - Value-coded:   \"E_54_@_V_161\" -> \"E_54=V_161\"\n",
    "    \"\"\"\n",
    "    base, value = split_ev(item)\n",
    "    return base if value is None else f\"{base}={value}\"\n",
    "\n",
    "\n",
    "print(\"✓ Evidence parsing helpers loaded\")\n",
    "print(\"  Functions: split_ev, is_effective_item, to_item_token\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1891413f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Deduplication helpers loaded\n",
      "  Functions: add_case_hash, remove_leakage_keep_test_then_val, dedup_with_frequency\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "===============================================================================\n",
    "1.1b: Deduplication & Leakage Detection Helpers\n",
    "===============================================================================\n",
    "Functions for case hashing, duplicate removal, and preventing data leakage.\n",
    "Based on EDA findings: ~2% duplicates, ~1,965 train/test overlaps.\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def add_case_hash(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create stable fingerprint for each case (for deduplication).\n",
    "    \n",
    "    Hash computed from: AGE, SEX, INITIAL_EVIDENCE, EVIDENCES (sorted)\n",
    "    Note: PATHOLOGY excluded - same symptoms can have different diagnoses.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with 'case_hash' column added\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Sort evidences for order-independence\n",
    "    df[\"EVIDENCES_sorted\"] = df[\"EVIDENCES_list\"].apply(lambda lst: tuple(sorted(lst)))\n",
    "    \n",
    "    # Hash based on input features only\n",
    "    key_cols = [\"AGE\", \"SEX\", \"INITIAL_EVIDENCE\", \"EVIDENCES_sorted\"]\n",
    "    df[\"case_hash\"] = pd.util.hash_pandas_object(df[key_cols], index=False).astype(\"uint64\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_leakage_keep_test_then_val(\n",
    "    train_df: pd.DataFrame,\n",
    "    val_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    key: str = \"case_hash_inputs\",\n",
    "    verbose: bool = True\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Remove cross-split duplicates (priority: test > val > train).\n",
    "    \n",
    "    Returns:\n",
    "        Cleaned (train, val, test) with no overlapping cases\n",
    "    \"\"\"\n",
    "    orig_train, orig_val = len(train_df), len(val_df)\n",
    "    \n",
    "    # Remove test cases from train and val\n",
    "    test_keys = set(test_df[key].unique())\n",
    "    train_df = train_df[~train_df[key].isin(test_keys)].copy()\n",
    "    val_df = val_df[~val_df[key].isin(test_keys)].copy()\n",
    "    \n",
    "    # Remove val cases from train\n",
    "    val_keys = set(val_df[key].unique())\n",
    "    train_df = train_df[~train_df[key].isin(val_keys)].copy()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  Leakage removal:\")\n",
    "        print(f\"    Train: {orig_train:,} → {len(train_df):,} (-{orig_train - len(train_df):,})\")\n",
    "        print(f\"    Val:   {orig_val:,} → {len(val_df):,} (-{orig_val - len(val_df):,})\")\n",
    "        print(f\"    Test:  {len(test_df):,} (unchanged)\")\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "def dedup_with_frequency(df: pd.DataFrame, key: str = \"case_hash\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove within-split duplicates, preserve frequency for sample weighting.\n",
    "    \n",
    "    Example:\n",
    "        Before: [Case A, Case A, Case B] → 3 rows\n",
    "        After:  [Case A (freq=2), Case B (freq=1)] → 2 rows\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"frequency\"] = df.groupby(key)[key].transform(\"size\")\n",
    "    df = df.drop_duplicates(subset=[key]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"✓ Deduplication helpers loaded\")\n",
    "print(\"  Functions: add_case_hash, remove_leakage_keep_test_then_val, dedup_with_frequency\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8fe44eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part creates an additional add_hash and deduplication helper functions\n",
    "# The goal: Create separate hashes for leakage control/deduplication between splits and deduplication within each split\n",
    "# Leakage control should not include target ('PATHOLOGY') column (uses the first def add_case_hash)\n",
    "# Within the same split hash function includes an additional 'PATHOLOGY' column \n",
    "\n",
    "def add_hashes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add two hashes:\n",
    "      - case_hash_inputs: used for leakage control (grouping by inputs only)\n",
    "      - case_hash_record: used for exact deduplication + frequency (inputs + PATHOLOGY)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # order-independent evidence representation\n",
    "    df[\"EVIDENCES_sorted\"] = df[\"EVIDENCES_list\"].apply(lambda lst: tuple(sorted(lst)))\n",
    "\n",
    "    input_cols = [\"AGE\", \"SEX\", \"INITIAL_EVIDENCE\", \"EVIDENCES_sorted\"]\n",
    "    df[\"case_hash_inputs\"] = pd.util.hash_pandas_object(df[input_cols], index=False).astype(\"uint64\")\n",
    "\n",
    "    record_cols = input_cols + [\"PATHOLOGY\"]\n",
    "    df[\"case_hash_record\"] = pd.util.hash_pandas_object(df[record_cols], index=False).astype(\"uint64\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def dedup_with_frequency_on_record(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Deduplicate exact duplicates (same inputs + same pathology),\n",
    "    and keep frequency as a training weight.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"frequency\"] = df.groupby(\"case_hash_record\")[\"case_hash_record\"].transform(\"size\")\n",
    "    df = df.drop_duplicates(\"case_hash_record\", keep=\"first\").reset_index(drop=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "905f9b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Feature engineering helper loaded\n",
      "  Function: build_effective_evidences\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "===============================================================================\n",
    "1.1c: Feature Engineering Helper\n",
    "===============================================================================\n",
    "Batch processing: filter default-valued items + create ML tokens.\n",
    "\n",
    "Key outputs:\n",
    "- EVIDENCES_effective_items: the filtered evidence entries (keeps value codes)\n",
    "- EVIDENCES_effective_tokens:\n",
    "    - \"base\"  -> unique base codes per case (e.g., [\"E_66\",\"E_204\",\"E_54\"])\n",
    "    - \"token\" -> value-level tokens (e.g., [\"E_66\",\"E_204=V_3\",\"E_54=V_161\"])\n",
    "- num_items_effective: number of effective ITEMS (keeps multi-choice values)\n",
    "- num_bases_effective: number of effective BASE CODES (collapses multi-choice)\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "\n",
    "def build_effective_evidences(\n",
    "    df: pd.DataFrame,\n",
    "    evidences_map: Dict,\n",
    "    encoding: str = \"base\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter defaults and create ML-ready tokens.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with column 'EVIDENCES_list'\n",
    "        evidences_map: mapping loaded from release_evidences.json\n",
    "        encoding:\n",
    "            - \"base\":  unique base codes per patient (recommended baseline)\n",
    "            - \"token\": value-level tokens (richer features, more columns)\n",
    "\n",
    "    Returns:\n",
    "        df copy with new columns:\n",
    "        - EVIDENCES_effective_items\n",
    "        - EVIDENCES_effective_tokens\n",
    "        - num_items_effective\n",
    "        - num_bases_effective\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1) Filter default-valued evidences (keep only informative items)\n",
    "    def filter_defaults(lst: List[str]) -> List[str]:\n",
    "        return [x for x in lst if is_effective_item(x, evidences_map)]\n",
    "\n",
    "    df[\"EVIDENCES_effective_items\"] = df[\"EVIDENCES_list\"].apply(filter_defaults)\n",
    "\n",
    "    # 2) Build tokens for ML\n",
    "    if encoding == \"base\":\n",
    "        # IMPORTANT: deduplicate base codes per patient\n",
    "        # (multi-choice can include the same base multiple times with different values)\n",
    "        df[\"EVIDENCES_effective_tokens\"] = df[\"EVIDENCES_effective_items\"].apply(\n",
    "            lambda lst: sorted({split_ev(x)[0] for x in lst})\n",
    "        )\n",
    "    elif encoding == \"token\":\n",
    "        # Keep value-level detail (still filtered for defaults)\n",
    "        # Deduplicate exact tokens just in case the raw list contains repeats\n",
    "        df[\"EVIDENCES_effective_tokens\"] = df[\"EVIDENCES_effective_items\"].apply(\n",
    "            lambda lst: list(dict.fromkeys(to_item_token(x) for x in lst))\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown encoding: {encoding}. Use 'base' or 'token'.\")\n",
    "\n",
    "    # 3) Counts\n",
    "    df[\"num_items_effective\"] = df[\"EVIDENCES_effective_items\"].apply(len)\n",
    "    df[\"num_bases_effective\"] = df[\"EVIDENCES_effective_items\"].apply(\n",
    "        lambda lst: len({split_ev(x)[0] for x in lst})\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"✓ Feature engineering helper loaded\")\n",
    "print(\"  Function: build_effective_evidences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26084902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DDXPlus dataset from Hugging Face...\n",
      "✓ Train: 1,025,602 rows\n",
      "✓ Val:   132,448 rows\n",
      "✓ Test:  134,529 rows\n",
      "\n",
      "Loading mapping files...\n",
      "✓ Evidence definitions:   223 (expected ~223)\n",
      "✓ Condition definitions:  49 (expected ~49)\n",
      "\n",
      "Dataset columns:\n",
      "['AGE', 'DIFFERENTIAL_DIAGNOSIS', 'SEX', 'PATHOLOGY', 'EVIDENCES', 'INITIAL_EVIDENCE']\n",
      "\n",
      "Sample row (selected columns):\n",
      "   AGE SEX PATHOLOGY INITIAL_EVIDENCE  \\\n",
      "0   18   M      URTI             E_91   \n",
      "\n",
      "                                                                                                                 EVIDENCES  \n",
      "0  ['E_48', 'E_50', 'E_53', 'E_54_@_V_161', 'E_54_@_V_183', 'E_55_@_V_89', 'E_55_@_V_108', 'E_55_@_V_167', 'E_56_@_4', ...  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1.2: Load Data\n",
    "Purpose: Load DDXPlus dataset splits and the official mapping files.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Loading DDXPlus dataset from Hugging Face...\")\n",
    "\n",
    "# Load dataset (first run can take longer)\n",
    "dataset = load_dataset(\"aai530-group6/ddxplus\")\n",
    "\n",
    "# Convert to pandas DataFrames\n",
    "df_train = dataset[\"train\"].to_pandas()\n",
    "df_val   = dataset[\"validate\"].to_pandas()\n",
    "df_test  = dataset[\"test\"].to_pandas()\n",
    "\n",
    "print(f\"✓ Train: {len(df_train):,} rows\")\n",
    "print(f\"✓ Val:   {len(df_val):,} rows\")\n",
    "print(f\"✓ Test:  {len(df_test):,} rows\")\n",
    "\n",
    "print(\"\\nLoading mapping files...\")\n",
    "\n",
    "# Evidence mapping (symptoms/findings with default values)\n",
    "with open(config.data_dir / \"release_evidences.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    evidences_map = json.load(f)\n",
    "\n",
    "# Condition mapping (pathologies)\n",
    "with open(config.data_dir / \"release_conditions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    conditions_map = json.load(f)\n",
    "\n",
    "print(f\"✓ Evidence definitions:   {len(evidences_map)} (expected ~223)\")\n",
    "print(f\"✓ Condition definitions:  {len(conditions_map)} (expected ~49)\")\n",
    "\n",
    "# Quick peek\n",
    "print(\"\\nDataset columns:\")\n",
    "print(df_train.columns.tolist())\n",
    "\n",
    "print(\"\\nSample row (selected columns):\")\n",
    "print(df_train.head(1)[[\"AGE\", \"SEX\", \"PATHOLOGY\", \"INITIAL_EVIDENCE\", \"EVIDENCES\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f63b51bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing string-encoded columns...\n",
      "✓ Parsed EVIDENCES → EVIDENCES_list\n",
      "✓ Parsed DIFFERENTIAL_DIAGNOSIS → DIFFERENTIAL_DIAGNOSIS_list\n",
      "\n",
      "Verification:\n",
      "  EVIDENCES_list: 19 items (type: list)\n",
      "  Sample: ['E_48', 'E_50', 'E_53']...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "===============================================================================\n",
    "Section 1.3: Parse String-Encoded Columns\n",
    "===============================================================================\n",
    "Convert EVIDENCES and DIFFERENTIAL_DIAGNOSIS from strings to Python lists.\n",
    "\n",
    "From EDA (Notebook 01, Section 2.2): These columns are stored as strings.\n",
    "Fix: Use ast.literal_eval() to convert to actual Python lists.\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "import ast\n",
    "\n",
    "print(\"Parsing string-encoded columns...\")\n",
    "\n",
    "# Parse EVIDENCES\n",
    "df_train['EVIDENCES_list'] = df_train['EVIDENCES'].apply(ast.literal_eval)\n",
    "df_val['EVIDENCES_list'] = df_val['EVIDENCES'].apply(ast.literal_eval)\n",
    "df_test['EVIDENCES_list'] = df_test['EVIDENCES'].apply(ast.literal_eval)\n",
    "\n",
    "# Parse DIFFERENTIAL_DIAGNOSIS\n",
    "df_train['DIFFERENTIAL_DIAGNOSIS_list'] = df_train['DIFFERENTIAL_DIAGNOSIS'].apply(ast.literal_eval)\n",
    "df_val['DIFFERENTIAL_DIAGNOSIS_list'] = df_val['DIFFERENTIAL_DIAGNOSIS'].apply(ast.literal_eval)\n",
    "df_test['DIFFERENTIAL_DIAGNOSIS_list'] = df_test['DIFFERENTIAL_DIAGNOSIS'].apply(ast.literal_eval)\n",
    "\n",
    "print(\"✓ Parsed EVIDENCES → EVIDENCES_list\")\n",
    "print(\"✓ Parsed DIFFERENTIAL_DIAGNOSIS → DIFFERENTIAL_DIAGNOSIS_list\")\n",
    "\n",
    "# Quick verification\n",
    "sample = df_train.iloc[0]\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"  EVIDENCES_list: {len(sample['EVIDENCES_list'])} items (type: {type(sample['EVIDENCES_list']).__name__})\")\n",
    "print(f\"  Sample: {sample['EVIDENCES_list'][:3]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e0608c",
   "metadata": {},
   "source": [
    "### Section 1.4 — Add Case Hashes (Fingerprints for Deduplication & Leakage Control)\n",
    "\n",
    "Two hashes are created per case to cleanly separate two distinct concerns:\n",
    "\n",
    "| Hash | Columns | Purpose |\n",
    "|------|---------|---------|\n",
    "| `case_hash_inputs` | `AGE, SEX, INITIAL_EVIDENCE, EVIDENCES_sorted` | Leakage control — same symptom presentation must not cross splits |\n",
    "| `case_hash_record` | `AGE, SEX, INITIAL_EVIDENCE, EVIDENCES_sorted, PATHOLOGY` | True duplicate removal — identical case AND identical diagnosis |\n",
    "\n",
    "**Why two hashes?**  \n",
    "Using inputs-only for deduplication would collapse cases where the same symptoms \n",
    "lead to different diagnoses — distorting class frequencies and introducing hidden \n",
    "label noise. Using the full record for leakage control would allow the same \n",
    "*presentation* to appear in both train and test under different labels.\n",
    "\n",
    "> **Note on DDXPlus:** Same-input/different-label cases likely reflect synthetic \n",
    "> generation artifacts rather than true clinical ambiguity — but the two-hash \n",
    "> approach is the more rigorous and generalizable design.\n",
    "\n",
    "From EDA: 2% duplicates within splits, 3,925 cross-split overlaps detected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e4efb564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding case hashes...\n",
      "✓ Case hashes added to all splits\n",
      "\n",
      "======================================================================\n",
      "DUPLICATE DETECTION (Within Splits)\n",
      "======================================================================\n",
      "Train  - Total: 1,025,602  |  Unique: 1,012,347  |  Duplicates: 13,255 (1.3%)\n",
      "Val    - Total: 132,448  |  Unique: 132,373  |  Duplicates: 75 (0.1%)\n",
      "Test   - Total: 134,529  |  Unique: 134,428  |  Duplicates: 101 (0.1%)\n",
      "\n",
      "======================================================================\n",
      "LEAKAGE DETECTION (Across Splits)\n",
      "======================================================================\n",
      "Train ↔ Val overlap:   1,788 cases\n",
      "Train ↔ Test overlap:  1,965 cases\n",
      "Val ↔ Test overlap:    172 cases\n",
      "Total leakage:         3,925 cases\n",
      "\n",
      "⚠️  Data leakage detected! Will be fixed in Section 1.5\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "===============================================================================\n",
    "Section 1.4: Add Case Hashes (Fingerprints for Deduplication)\n",
    "===============================================================================\n",
    "Create stable fingerprints for each case to enable:\n",
    "- Duplicate detection within splits\n",
    "- Data leakage detection across splits\n",
    "\n",
    "2 Hashes are computed for leakage control/deduplication between the splits and for deduplication within the splits:\n",
    "- case_hash_inputs: AGE, SEX, INITIAL_EVIDENCE, EVIDENCES (sorted for order-independence)\n",
    "- Note: PATHOLOGY excluded for leakage control - same symptoms can have different diagnoses\n",
    "- case_hash_records:  AGE, SEX, INITIAL_EVIDENCE, EVIDENCES (sorted for order-independence), PATHOLOGY\n",
    "\n",
    "From EDA: Found ~2% duplicates and ~3,925 cross-split overlaps.\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "\n",
    "print(\"Adding case hashes...\")\n",
    "\n",
    "# Add hash to each split\n",
    "df_train = add_hashes(df_train)\n",
    "df_val = add_hashes(df_val)\n",
    "df_test = add_hashes(df_test)\n",
    "\n",
    "print(\"✓ Case hashes added to all splits\")\n",
    "\n",
    "# Verification: Check for duplicates within each split: record level\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DUPLICATE DETECTION (Within Splits)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, df in [('Train', df_train), ('Val', df_val), ('Test', df_test)]:\n",
    "    total = len(df)\n",
    "    unique = df['case_hash_record'].nunique()\n",
    "    duplicates = total - unique\n",
    "    dup_pct = (duplicates / total) * 100\n",
    "    \n",
    "    print(f\"{name:6} - Total: {total:,}  |  Unique: {unique:,}  |  Duplicates: {duplicates:,} ({dup_pct:.1f}%)\")\n",
    "\n",
    "# Check for leakage across splits:input level\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LEAKAGE DETECTION (Across Splits)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "train_hash_inputs = set(df_train['case_hash_inputs'].unique())\n",
    "val_hash_inputs = set(df_val['case_hash_inputs'].unique())\n",
    "test_hash_inputs = set(df_test['case_hash_inputs'].unique())\n",
    "\n",
    "train_val_overlap = len(train_hash_inputs & val_hash_inputs)\n",
    "train_test_overlap = len(train_hash_inputs & test_hash_inputs)\n",
    "val_test_overlap = len(val_hash_inputs & test_hash_inputs)\n",
    "\n",
    "print(f\"Train ↔ Val overlap:   {train_val_overlap:,} cases\")\n",
    "print(f\"Train ↔ Test overlap:  {train_test_overlap:,} cases\")\n",
    "print(f\"Val ↔ Test overlap:    {val_test_overlap:,} cases\")\n",
    "print(f\"Total leakage:         {train_val_overlap + train_test_overlap + val_test_overlap:,} cases\")\n",
    "\n",
    "if train_test_overlap > 0:\n",
    "    print(\"\\n⚠️  Data leakage detected! Will be fixed in Section 1.5\")\n",
    "else:\n",
    "    print(\"\\n✓ No data leakage detected\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b86f9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing data leakage across splits...\n",
      "\n",
      "Before leakage removal:\n",
      "  Train: 1,025,602 cases\n",
      "  Val:   132,448 cases\n",
      "  Test:  134,529 cases\n",
      "  Leakage removal:\n",
      "    Train: 1,025,602 → 1,020,977 (-4,625)\n",
      "    Val:   132,448 → 132,274 (-174)\n",
      "    Test:  134,529 (unchanged)\n",
      "\n",
      "After leakage removal:\n",
      "  Train: 1,020,977 cases\n",
      "  Val:   132,274 cases\n",
      "  Test:  134,529 cases\n",
      "\n",
      "======================================================================\n",
      "VERIFICATION: Checking for remaining leakage...\n",
      "======================================================================\n",
      "Train ↔ Val overlap:   0 cases\n",
      "Train ↔ Test overlap:  0 cases\n",
      "Val ↔ Test overlap:    0 cases\n",
      "\n",
      "✅ SUCCESS: All data leakage removed!\n",
      "   Splits are now completely disjoint.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "===============================================================================\n",
    "Section 1.5: Remove Data Leakage (Cross-Split Duplicates)\n",
    "===============================================================================\n",
    "Remove cases that appear in multiple splits to prevent data leakage.\n",
    "\n",
    "Priority order (most critical first):\n",
    "1. Keep TEST unchanged (most important for unbiased evaluation)\n",
    "2. Remove TEST cases from TRAIN and VAL\n",
    "3. Keep VAL (after removing test overlaps)\n",
    "4. Remove VAL cases from TRAIN\n",
    "\n",
    "From Section 1.4: Detected 3,925 leaking cases\n",
    "- Train ↔ Test: 1,965 cases (CRITICAL - corrupts final evaluation)\n",
    "- Train ↔ Val:  1,788 cases (biases validation)\n",
    "- Val ↔ Test:   172 cases (minor but should fix)\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "\n",
    "print(\"Removing data leakage across splits...\")\n",
    "print(f\"\\nBefore leakage removal:\")\n",
    "print(f\"  Train: {len(df_train):,} cases\")\n",
    "print(f\"  Val:   {len(df_val):,} cases\")\n",
    "print(f\"  Test:  {len(df_test):,} cases\")\n",
    "\n",
    "# Remove leakage (uses helper from Cell 1.1b)\n",
    "df_train, df_val, df_test = remove_leakage_keep_test_then_val(\n",
    "    df_train, \n",
    "    df_val, \n",
    "    df_test,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nAfter leakage removal:\")\n",
    "print(f\"  Train: {len(df_train):,} cases\")\n",
    "print(f\"  Val:   {len(df_val):,} cases\")\n",
    "print(f\"  Test:  {len(df_test):,} cases\")\n",
    "\n",
    "# Verify: Check that leakage is gone\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VERIFICATION: Checking for remaining leakage...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "train_hashes = set(df_train['case_hash_inputs'].unique())\n",
    "val_hashes = set(df_val['case_hash_inputs'].unique())\n",
    "test_hashes = set(df_test['case_hash_inputs'].unique())\n",
    "\n",
    "train_val_overlap = len(train_hashes & val_hashes)\n",
    "train_test_overlap = len(train_hashes & test_hashes)\n",
    "val_test_overlap = len(val_hashes & test_hashes)\n",
    "\n",
    "print(f\"Train ↔ Val overlap:   {train_val_overlap:,} cases\")\n",
    "print(f\"Train ↔ Test overlap:  {train_test_overlap:,} cases\")\n",
    "print(f\"Val ↔ Test overlap:    {val_test_overlap:,} cases\")\n",
    "\n",
    "if train_val_overlap == 0 and train_test_overlap == 0 and val_test_overlap == 0:\n",
    "    print(\"\\n✅ SUCCESS: All data leakage removed!\")\n",
    "    print(\"   Splits are now completely disjoint.\")\n",
    "else:\n",
    "    print(\"\\n⚠️  WARNING: Some leakage remains!\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad595d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing duplicates within splits...\n",
      "\n",
      "✓ Config: use_unique_cases = True (removing duplicates)\n",
      "\n",
      "Deduplication results:\n",
      "  Train: 1,020,977 → 1,008,626 (removed 12,351, 1.21%)\n",
      "  Val:   132,274 → 132,201 (removed 73, 0.06%)\n",
      "  Test:  134,529 → 134,428 (removed 101, 0.08%)\n",
      "\n",
      "======================================================================\n",
      "FREQUENCY DISTRIBUTION\n",
      "======================================================================\n",
      "\n",
      "Train:\n",
      "  Unique patterns (freq=1):     998,350 (99.0%)\n",
      "  Duplicate patterns (freq>1):  10,276 (1.0%)\n",
      "  Max frequency:                6 (same case appeared 6x)\n",
      "\n",
      "Val:\n",
      "  Unique patterns (freq=1):     132,128 (99.9%)\n",
      "  Duplicate patterns (freq>1):  73 (0.1%)\n",
      "  Max frequency:                2 (same case appeared 2x)\n",
      "  Frequency breakdown: {1: np.int64(132128), 2: np.int64(73)}\n",
      "\n",
      "Test:\n",
      "  Unique patterns (freq=1):     134,327 (99.9%)\n",
      "  Duplicate patterns (freq>1):  101 (0.1%)\n",
      "  Max frequency:                2 (same case appeared 2x)\n",
      "  Frequency breakdown: {1: np.int64(134327), 2: np.int64(101)}\n",
      "\n",
      "======================================================================\n",
      "✓ Duplicates removed, frequency information preserved\n",
      "  Use config.use_sample_weights=True to leverage frequency during training\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "===============================================================================\n",
    "Section 1.6: Deduplicate Within Splits (Preserve Frequency Information)\n",
    "===============================================================================\n",
    "Remove duplicate cases within each split while preserving occurrence counts.\n",
    "\n",
    "From Section 1.4: Found duplicates within splits:\n",
    "- Train: 13,255 duplicates (1.3%)\n",
    "- Val:   75 duplicates (0.1%)\n",
    "- Test:  101 duplicates (0.1%)\n",
    "\n",
    "Strategy:\n",
    "1. Count how many times each case pattern appears (frequency)\n",
    "2. Keep only one copy of each pattern\n",
    "3. Use frequency as sample_weight during training\n",
    "\n",
    "This preserves information about pattern importance without actual duplicates.\n",
    "\n",
    "Controlled by: config.use_unique_cases = {config.use_unique_cases}\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "\n",
    "print(\"Processing duplicates within splits...\")\n",
    "\n",
    "if config.use_unique_cases:\n",
    "    print(\"\\n✓ Config: use_unique_cases = True (removing duplicates)\\n\")\n",
    "    \n",
    "    # Store original counts\n",
    "    orig_train, orig_val, orig_test = len(df_train), len(df_val), len(df_test)\n",
    "    \n",
    "    # Deduplicate each split (adds 'frequency' column)\n",
    "    df_train = dedup_with_frequency_on_record(df_train)\n",
    "    df_val = dedup_with_frequency_on_record(df_val)\n",
    "    df_test = dedup_with_frequency_on_record(df_test)\n",
    "    \n",
    "    # Calculate removed counts\n",
    "    removed_train = orig_train - len(df_train)\n",
    "    removed_val = orig_val - len(df_val)\n",
    "    removed_test = orig_test - len(df_test)\n",
    "    \n",
    "    print(f\"Deduplication results:\")\n",
    "    print(f\"  Train: {orig_train:,} → {len(df_train):,} (removed {removed_train:,}, {removed_train/orig_train*100:.2f}%)\")\n",
    "    print(f\"  Val:   {orig_val:,} → {len(df_val):,} (removed {removed_val:,}, {removed_val/orig_val*100:.2f}%)\")\n",
    "    print(f\"  Test:  {orig_test:,} → {len(df_test):,} (removed {removed_test:,}, {removed_test/orig_test*100:.2f}%)\")\n",
    "    \n",
    "    # Show frequency distribution\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FREQUENCY DISTRIBUTION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for name, df in [('Train', df_train), ('Val', df_val), ('Test', df_test)]:\n",
    "        freq_dist = df['frequency'].value_counts().sort_index()\n",
    "        unique_cases = (df['frequency'] == 1).sum()\n",
    "        duplicate_cases = (df['frequency'] > 1).sum()\n",
    "        \n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Unique patterns (freq=1):     {unique_cases:,} ({unique_cases/len(df)*100:.1f}%)\")\n",
    "        print(f\"  Duplicate patterns (freq>1):  {duplicate_cases:,} ({duplicate_cases/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        if duplicate_cases > 0:\n",
    "            max_freq = df['frequency'].max()\n",
    "            print(f\"  Max frequency:                {max_freq} (same case appeared {max_freq}x)\")\n",
    "            \n",
    "            # Show distribution\n",
    "            if len(freq_dist) <= 5:\n",
    "                print(f\"  Frequency breakdown: {dict(freq_dist)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"✓ Duplicates removed, frequency information preserved\")\n",
    "    print(\"  Use config.use_sample_weights=True to leverage frequency during training\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n✗ Config: use_unique_cases = False (keeping all duplicates)\\n\")\n",
    "    \n",
    "    # Add frequency = 1 for all cases\n",
    "    df_train['frequency'] = 1\n",
    "    df_val['frequency'] = 1\n",
    "    df_test['frequency'] = 1\n",
    "    \n",
    "    print(f\"Keeping all cases:\")\n",
    "    print(f\"  Train: {len(df_train):,} cases (no deduplication)\")\n",
    "    print(f\"  Val:   {len(df_val):,} cases (no deduplication)\")\n",
    "    print(f\"  Test:  {len(df_test):,} cases (no deduplication)\")\n",
    "    print(f\"\\nAll frequency values set to 1 (uniform weighting)\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c5f0e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split distribution after de-duplication: \n",
      "\n",
      "Train: 1008626 rows; 79.09% of the dataset\n",
      "Validation: 132201 rows; 10.37% of the dataset\n",
      "Test: 134428 rows; 10.54% of the dataset\n",
      "\n",
      "Total: 1275255 rows; 100% of the dataset\n"
     ]
    }
   ],
   "source": [
    "total_length = len(df_train)+len(df_val)+len(df_test)\n",
    "print(\"Split distribution after de-duplication: \")\n",
    "print(f\"\\nTrain: {len(df_train)} rows; {((len(df_train)/total_length)*100):.2f}% of the dataset\")\n",
    "print(f\"Validation: {len(df_val)} rows; {((len(df_val)/total_length)*100):.2f}% of the dataset\")\n",
    "print(f\"Test: {len(df_test)} rows; {((len(df_test)/total_length)*100):.2f}% of the dataset\")\n",
    "print(f\"\\nTotal: {total_length} rows; 100% of the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65a7abee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing evidence lists...\n",
      "✓ Config: filter_defaults = True\n",
      "✓ Config: feature_encoding = base\n",
      "\n",
      "======================================================================\n",
      "BEFORE FILTERING (Raw Evidence Item Counts)\n",
      "======================================================================\n",
      "Train  - Mean raw evidence ITEMS: 19.90\n",
      "Val    - Mean raw evidence ITEMS: 20.16\n",
      "Test   - Mean raw evidence ITEMS: 20.06\n",
      "\n",
      "======================================================================\n",
      "APPLYING DEFAULT FILTERING + TOKENIZATION\n",
      "======================================================================\n",
      "✓ Filtering applied to Train / Val / Test\n",
      "\n",
      "======================================================================\n",
      "AFTER FILTERING (Effective Evidence Counts)\n",
      "======================================================================\n",
      "\n",
      "Train:\n",
      "  Raw mean (items):          19.90\n",
      "  Effective mean (items):    17.74   (after filtering defaults)\n",
      "  Effective mean (base):     13.31   (unique base codes, after filtering)\n",
      "  Defaults filtered (items): 10.8%\n",
      "\n",
      "Val:\n",
      "  Raw mean (items):          20.16\n",
      "  Effective mean (items):    17.97   (after filtering defaults)\n",
      "  Effective mean (base):     13.44   (unique base codes, after filtering)\n",
      "  Defaults filtered (items): 10.8%\n",
      "\n",
      "Test:\n",
      "  Raw mean (items):          20.06\n",
      "  Effective mean (items):    17.89   (after filtering defaults)\n",
      "  Effective mean (base):     13.40   (unique base codes, after filtering)\n",
      "  Defaults filtered (items): 10.8%\n",
      "\n",
      "======================================================================\n",
      "VALIDATION: Weighted vs Unweighted (Unique-pattern vs Original-distribution view)\n",
      "======================================================================\n",
      "Unweighted mean BASE (unique patterns view): 13.33\n",
      "Weighted mean BASE   (original distribution): 13.26\n",
      "Unweighted mean ITEMS: 17.78\n",
      "Weighted mean ITEMS:   17.67\n",
      "\n",
      "Reference (from the EDA):\n",
      "  - BASE weighted mean ~13.56 (paper-aligned)\n",
      "  - BASE unweighted mean ~13.65 (unique patterns)\n",
      "  - ITEMS mean ~17.95\n",
      "\n",
      "⚠️ BASE weighted mean is outside expected range — check default filtering logic.\n",
      "\n",
      "======================================================================\n",
      "TOKEN SANITY CHECK\n",
      "======================================================================\n",
      "Encoding type: base\n",
      "Example tokens (first 8): ['E_201', 'E_222', 'E_48', 'E_50', 'E_53', 'E_54', 'E_55', 'E_56']\n",
      "Unique tokens observed in first 10k train rows: 223\n",
      "Evidence base codes in mapping file: 223\n",
      "✓ Looks consistent for base encoding.\n",
      "======================================================================\n",
      "\n",
      "✅ Section 1.7 Complete\n",
      "Columns available for ML:\n",
      "  - EVIDENCES_effective_items\n",
      "  - EVIDENCES_effective_tokens\n",
      "  - num_items_effective\n",
      "  - num_bases_effective\n",
      "  - frequency (if deduplicated)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "===============================================================================\n",
    "Section 1.7: Filter Default Values + Validate Evidence Counts\n",
    "===============================================================================\n",
    "Goal:\n",
    "- Create ML-ready evidence features based on EDA findings.\n",
    "\n",
    "IMPORTANT: There are TWO different \"evidence count\" concepts:\n",
    "\n",
    "1) ITEM-level count (num_items_effective)\n",
    "   - Counts every effective evidence entry kept after filtering defaults.\n",
    "   - Multi-choice questions can contribute multiple items for the same base.\n",
    "\n",
    "2) BASE-level count (num_bases_effective)\n",
    "   - Counts unique evidence base codes after filtering defaults.\n",
    "   - This is the number that matches the published \"average number of evidences\"\n",
    "     (because it treats one question as one concept).\n",
    "\n",
    "What to expect (from your EDA):\n",
    "- Raw items mean ~19.77\n",
    "- Effective BASE mean:\n",
    "    - ~13.56 when weighted to match the original dataset distribution\n",
    "    - ~13.65 on unique case patterns (deduplicated view)\n",
    "- Effective ITEM mean ~17.95 (because multi-choice adds extra values)\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "\n",
    "print(\"Processing evidence lists...\")\n",
    "\n",
    "# Safety: if filter_defaults not defined in config\n",
    "if not hasattr(config, \"filter_defaults\"):\n",
    "    config.filter_defaults = True\n",
    "    print(\"⚠️ config.filter_defaults not set, defaulting to True\")\n",
    "\n",
    "print(f\"✓ Config: filter_defaults = {config.filter_defaults}\")\n",
    "print(f\"✓ Config: feature_encoding = {config.feature_encoding}\")\n",
    "\n",
    "# -----------------------------\n",
    "# BEFORE filtering (raw counts)\n",
    "# -----------------------------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BEFORE FILTERING (Raw Evidence Item Counts)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, df in [(\"Train\", df_train), (\"Val\", df_val), (\"Test\", df_test)]:\n",
    "    mean_raw_items = df[\"EVIDENCES_list\"].apply(len).mean()\n",
    "    print(f\"{name:6} - Mean raw evidence ITEMS: {mean_raw_items:.2f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Apply filtering + tokenization\n",
    "# -----------------------------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"APPLYING DEFAULT FILTERING + TOKENIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if config.filter_defaults:\n",
    "    df_train = build_effective_evidences(df_train, evidences_map, config.feature_encoding)\n",
    "    df_val   = build_effective_evidences(df_val, evidences_map, config.feature_encoding)\n",
    "    df_test  = build_effective_evidences(df_test, evidences_map, config.feature_encoding)\n",
    "    print(\"✓ Filtering applied to Train / Val / Test\")\n",
    "else:\n",
    "    # Still create consistent columns even if you skip filtering\n",
    "    for df in (df_train, df_val, df_test):\n",
    "        df[\"EVIDENCES_effective_items\"]  = df[\"EVIDENCES_list\"]\n",
    "        df[\"num_items_effective\"] = df[\"EVIDENCES_list\"].apply(len)\n",
    "        df[\"num_bases_effective\"] = df[\"EVIDENCES_list\"].apply(lambda lst: len({split_ev(x)[0] for x in lst}))\n",
    "        if config.feature_encoding == \"base\":\n",
    "            df[\"EVIDENCES_effective_tokens\"] = df[\"EVIDENCES_list\"].apply(lambda lst: sorted({split_ev(x)[0] for x in lst}))\n",
    "        else:\n",
    "            df[\"EVIDENCES_effective_tokens\"] = df[\"EVIDENCES_list\"].apply(lambda lst: list(dict.fromkeys(to_item_token(x) for x in lst)))\n",
    "    print(\"⚠️ Filtering skipped (config.filter_defaults = False)\")\n",
    "\n",
    "# -----------------------------\n",
    "# AFTER filtering (report both)\n",
    "# -----------------------------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"AFTER FILTERING (Effective Evidence Counts)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, df in [(\"Train\", df_train), (\"Val\", df_val), (\"Test\", df_test)]:\n",
    "    mean_raw_items = df[\"EVIDENCES_list\"].apply(len).mean()\n",
    "    mean_eff_items = df[\"num_items_effective\"].mean()\n",
    "    mean_eff_bases = df[\"num_bases_effective\"].mean()\n",
    "\n",
    "    red_items = ((mean_raw_items - mean_eff_items) / mean_raw_items) * 100\n",
    "\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Raw mean (items):          {mean_raw_items:.2f}\")\n",
    "    print(f\"  Effective mean (items):    {mean_eff_items:.2f}   (after filtering defaults)\")\n",
    "    print(f\"  Effective mean (base):     {mean_eff_bases:.2f}   (unique base codes, after filtering)\")\n",
    "    print(f\"  Defaults filtered (items): {red_items:.1f}%\")\n",
    "\n",
    "# -----------------------------\n",
    "# Weighted vs unweighted means\n",
    "# -----------------------------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VALIDATION: Weighted vs Unweighted (Unique-pattern vs Original-distribution view)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def weighted_mean(values: pd.Series, weights: pd.Series) -> float:\n",
    "    return float(np.average(values.astype(float), weights=weights.astype(float)))\n",
    "\n",
    "# Combine splits (useful for global comparison)\n",
    "df_all = pd.concat([df_train, df_val, df_test], ignore_index=True)\n",
    "\n",
    "if \"frequency\" in df_all.columns:\n",
    "    wm_base  = weighted_mean(df_all[\"num_bases_effective\"], df_all[\"frequency\"])\n",
    "    wm_items = weighted_mean(df_all[\"num_items_effective\"], df_all[\"frequency\"])\n",
    "else:\n",
    "    wm_base  = df_all[\"num_bases_effective\"].mean()\n",
    "    wm_items = df_all[\"num_items_effective\"].mean()\n",
    "\n",
    "um_base  = df_all[\"num_bases_effective\"].mean()\n",
    "um_items = df_all[\"num_items_effective\"].mean()\n",
    "\n",
    "print(f\"Unweighted mean BASE (unique patterns view): {um_base:.2f}\")\n",
    "print(f\"Weighted mean BASE   (original distribution): {wm_base:.2f}\")\n",
    "print(f\"Unweighted mean ITEMS: {um_items:.2f}\")\n",
    "print(f\"Weighted mean ITEMS:   {wm_items:.2f}\")\n",
    "\n",
    "print(\"\\nReference (from the EDA):\")\n",
    "print(\"  - BASE weighted mean ~13.56 (paper-aligned)\")\n",
    "print(\"  - BASE unweighted mean ~13.65 (unique patterns)\")\n",
    "print(\"  - ITEMS mean ~17.95\")\n",
    "\n",
    "# Quick sanity check ranges (not strict)\n",
    "if 13.3 <= wm_base <= 13.9:\n",
    "    print(\"\\n✅ BASE weighted mean is in a reasonable range.\")\n",
    "else:\n",
    "    print(\"\\n⚠️ BASE weighted mean is outside expected range — check default filtering logic.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Token sanity check (fast)\n",
    "# -----------------------------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TOKEN SANITY CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "sample_tokens = df_train.iloc[0][\"EVIDENCES_effective_tokens\"]\n",
    "print(f\"Encoding type: {config.feature_encoding}\")\n",
    "print(f\"Example tokens (first 8): {sample_tokens[:8]}\")\n",
    "\n",
    "# Check that token vocab roughly matches expectations without scanning all rows\n",
    "sample_vocab = set()\n",
    "for tokens in df_train[\"EVIDENCES_effective_tokens\"].head(10000):\n",
    "    sample_vocab.update(tokens)\n",
    "\n",
    "print(f\"Unique tokens observed in first 10k train rows: {len(sample_vocab)}\")\n",
    "\n",
    "if config.feature_encoding == \"base\":\n",
    "    print(f\"Evidence base codes in mapping file: {len(evidences_map)}\")\n",
    "    if len(sample_vocab) <= len(evidences_map):\n",
    "        print(\"✓ Looks consistent for base encoding.\")\n",
    "    else:\n",
    "        print(\"⚠️ More codes than mapping suggests — check parsing.\")\n",
    "else:\n",
    "    print(\"✓ Token encoding usually has more features than base encoding (value-level detail).\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\\n✅ Section 1.7 Complete\")\n",
    "print(\"Columns available for ML:\")\n",
    "print(\"  - EVIDENCES_effective_items\")\n",
    "print(\"  - EVIDENCES_effective_tokens\")\n",
    "print(\"  - num_items_effective\")\n",
    "print(\"  - num_bases_effective\")\n",
    "print(\"  - frequency (if deduplicated)\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a61359f",
   "metadata": {},
   "source": [
    "**Observation:** there is a large difference between EDA numbers for effective evidences (raw, items and bases). This requires investigation to see whether out filtering is too agressive, and we are filtering incorrectly or something has happenned with the original file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c865355d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[train]\n",
      "  mean raw items:    19.9\n",
      "  mean eff items:    17.74\n",
      "  mean eff bases:    13.31\n",
      "  mean dropped defaults (items): 2.16\n",
      "  drop rate: 0.108\n",
      "\n",
      "[validate]\n",
      "  mean raw items:    20.16\n",
      "  mean eff items:    17.97\n",
      "  mean eff bases:    13.44\n",
      "  mean dropped defaults (items): 2.19\n",
      "  drop rate: 0.108\n",
      "\n",
      "[test]\n",
      "  mean raw items:    20.06\n",
      "  mean eff items:    17.89\n",
      "  mean eff bases:    13.4\n",
      "  mean dropped defaults (items): 2.17\n",
      "  drop rate: 0.108\n"
     ]
    }
   ],
   "source": [
    "# Quick diagnostics on effective filtering impact\n",
    "for split_name, df in [(\"train\", df_train), (\"validate\", df_val), (\"test\", df_test)]:\n",
    "    if \"n_evidence_items_raw\" in df.columns:\n",
    "        raw_items = df[\"n_evidence_items_raw\"]\n",
    "    else:\n",
    "        raw_items = df[\"EVIDENCES_list\"].apply(len)\n",
    "\n",
    "    eff_items = df[\"num_items_effective\"]\n",
    "    eff_bases = df[\"num_bases_effective\"]\n",
    "\n",
    "    dropped = (raw_items - eff_items)\n",
    "\n",
    "    print(f\"\\n[{split_name}]\")\n",
    "    print(\"  mean raw items:   \", raw_items.mean().round(2))\n",
    "    print(\"  mean eff items:   \", eff_items.mean().round(2))\n",
    "    print(\"  mean eff bases:   \", eff_bases.mean().round(2))\n",
    "    print(\"  mean dropped defaults (items):\", dropped.mean().round(2))\n",
    "    print(\"  drop rate:\", (dropped.mean() / raw_items.mean()).round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94b932ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suspicious bases where dropped values != default_value: 0\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "\n",
    "def canonical(v: str) -> str:\n",
    "    \"\"\"Make 'V_0' and '0' comparable.\"\"\"\n",
    "    s = str(v)\n",
    "    return s[2:] if s.startswith(\"V_\") else s\n",
    "\n",
    "def check_default_filtering(df, evidences_map, n_rows=20000, seed=42):\n",
    "    rng = random.Random(seed)\n",
    "    idx = rng.sample(range(len(df)), k=min(n_rows, len(df)))\n",
    "    df_s = df.iloc[idx]\n",
    "\n",
    "    dropped_values = defaultdict(Counter)\n",
    "\n",
    "    for lst in df_s[\"EVIDENCES_list\"]:\n",
    "        for item in lst:\n",
    "            base, value = split_ev(item)\n",
    "            if value is None:\n",
    "                continue\n",
    "\n",
    "            default = evidences_map.get(base, {}).get(\"default_value\", None)\n",
    "            if default is None:\n",
    "                continue\n",
    "\n",
    "            # if your code drops it, record which value got dropped\n",
    "            if not is_effective_item(item, evidences_map):\n",
    "                dropped_values[base][value] += 1\n",
    "\n",
    "    # Find bases where you dropped values OTHER than the default (after canonical normalization)\n",
    "    suspicious = []\n",
    "    for base, counter in dropped_values.items():\n",
    "        default = evidences_map.get(base, {}).get(\"default_value\", None)\n",
    "        if default is None:\n",
    "            continue\n",
    "        allowed = {canonical(default)}\n",
    "        observed = {canonical(v) for v in counter.keys()}\n",
    "        if not observed.issubset(allowed):\n",
    "            suspicious.append((base, default, list(counter.items())[:5]))\n",
    "\n",
    "    print(\"Suspicious bases where dropped values != default_value:\", len(suspicious))\n",
    "    if suspicious:\n",
    "        print(\"Example suspicious entries (first 10):\")\n",
    "        for s in suspicious[:10]:\n",
    "            print(s)\n",
    "\n",
    "check_default_filtering(df_train, evidences_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d9c4230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence map size: 223\n",
      "Num evidences with default_value: 223\n",
      "E_204 default_value = V_10\n",
      "E_58 default_value = 0\n",
      "E_210 default_value = 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Evidence map size:\", len(evidences_map))\n",
    "defaults = [v.get(\"default_value\", None) for v in evidences_map.values()]\n",
    "print(\"Num evidences with default_value:\", sum(d is not None for d in defaults))\n",
    "\n",
    "# Quick spot-check a few known codes (edit codes you know from your analysis)\n",
    "for code in [\"E_204\", \"E_58\", \"E_210\"]:\n",
    "    if code in evidences_map:\n",
    "        print(code, \"default_value =\", evidences_map[code].get(\"default_value\"))\n",
    "    else:\n",
    "        print(code, \"not found in mapping!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ef6a6129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATHOLOGY\n",
      "1    1275255\n",
      "Name: count, dtype: int64\n",
      "Fraction of input signatures with >1 pathology: 0.0\n"
     ]
    }
   ],
   "source": [
    "# How many different pathologies share the same input signature?\n",
    "n_labels_per_input = df_all.groupby(\"case_hash_inputs\")[\"PATHOLOGY\"].nunique()\n",
    "\n",
    "print(n_labels_per_input.value_counts().head(10))\n",
    "print(\"Fraction of input signatures with >1 pathology:\",\n",
    "      (n_labels_per_input > 1).mean().round(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8193798",
   "metadata": {},
   "source": [
    "### Preprocessing Summary (Notebook 02)\n",
    "\n",
    "In this section, we prepared the DDXPlus dataset for machine learning:\n",
    "\n",
    "- Loaded train/validation/test splits and official mapping files (evidences + conditions).\n",
    "- Parsed string-encoded columns into Python lists:\n",
    "  - `EVIDENCES_list`\n",
    "  - `DIFFERENTIAL_DIAGNOSIS_list`\n",
    "- Created an input-based `case_hash` to detect duplicates and prevent cross-split leakage.\n",
    "- Removed leakage across splits (priority: keep test unchanged).\n",
    "- Deduplicated within each split while preserving `frequency` for optional sample weighting.\n",
    "- Filtered default-valued evidence items using the official `default_value` for each evidence.\n",
    "- Created ML-ready evidence tokens:\n",
    "  - Base encoding (`E_XX`) or token encoding (`E_XX=V_YY`)\n",
    "- Added evidence complexity features:\n",
    "  - `num_items_effective` (effective evidence *items*, keeps multi-choice values)\n",
    "  - `num_bases_effective` (effective evidence *base codes*, collapses multi-choice)\n",
    "- Validated preprocessing by comparing weighted vs unweighted evidence statistics.\n",
    "\n",
    "Result: the dataset is leakage-safe, deduplicated (optional), and has evidence features ready for vectorization and baseline model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af943a1f",
   "metadata": {},
   "source": [
    "## Phase 2: Feature Engineering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
